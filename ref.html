<html lang="en">
<head>
	<meta charset="utf-8">
	<title>NBC Actions Analysis</title>
	<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-colorschemes"></script>
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    MathJax = {
    	tex: {
        	inlineMath: [['$', '$'], ['\\(', '\\)']]
    	},
		svg: {
			fontCache: 'global'
		}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
	<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
	<link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<style>
html, body {
	font-family: 'Open Sans', sans-serif;
	width: 100%;
	height: 100%;
}
p {
	font-size: 1.3em;
}
.head {
	padding-top: 50px;
	padding-bottom: 10px;
	padding-left: 30px;
	position: relative;
	width: 100%;
}
h1 {
	text-transform: uppercase;
	letter-spacing: 1px;
	font-size: 3.5em;
}
h2 {
	font-weight: bold;
	font-size: 1.8em;
}
h3 {
	font-weight: bold;
	font-size: 1.5em;
}
h4 {
	font-weight: bold;
	margin-top: 20px;
}
.page-footer {
	padding: 50px;
	background-color: #eeeeee;
}
video {
	max-width: 100%;
	height: auto;
}
</style>
<body>
<div class="container">
	<div class="head">
		<h1>NBC Actions Analysis</h1>
	</div>
	<div class="panel-group">
		<p>This write-up summarizes statistical analyses on approximated latent action states and
			their cooccurrence with words, as a naive unsupervised approach to grounding words
			in actions.</p>
		<div class="panel panel-default">
			<div class="panel-heading"><h2>Actions</h2></div>
			<div class="panel-body">
				<p>Last episode on New Brown Corpus, we used a bayesian model, BP-AR-HMM, to segment our continuous spatial data into discrete action sequences.
					The inputs are hand position (3d euclidean) and rotation
					(4d quaternion). Only the hand with the greatest position variance for each
					session is used, as a proxy for dominant hand.</p>
				<p>The distribution of actions, after filtering out actions with fewer than 50 occurrences, is:</p>
				<div style="height: 300px">
					<canvas id="actionDistributionCanvas" width="400" height="400"></canvas>
				</div>
				<p>We want to incorporate some information about actions as they relate to objects
					in the world. However, we don't want to explicitly identify objects for the time being. So,
					each action is split into 2, based on whether that action trajectory
					ends on any object, or not. If an action ends on an object, an offset of +18 is added to the action id.
					This results in the following action distribution:</p>
					<div style="height: 300px">
						<canvas id="endDistributionCanvas" width="400" height="400"></canvas>
					</div>
			</div>
		</div>
		<div class="panel panel-default">
			<div class="panel-heading"><h2>Words</h2></div>
			<div class="panel-body">
				<p>Alongside actions are all lemmatized words occurring at least 20 times in our dataset.
					The counts of the 15 most frequent word lemmas are:</p>
				<div style="height: 300px">
					<canvas id="wordsCanvas" width="400" height="400"></canvas>
				</div>
				<p>Additionally, the counts of words from our list of target words are:</p>
				<div style="height: 300px">
					<canvas id="ourWordsCanvas" width="400" height="400"></canvas>
				</div>
			</div>
		</div>
		<div class="panel panel-default">
			<div class="panel-heading"><h2>Cooccurrence</h2></div>
			<div class="panel-body">
				<p>The goal is to learn unsupervisedly what verbs mean. This can be formulated as learning
					$A\longleftrightarrow V$, a bidirectional mapping between action space $A$ and word space
					(or vocab) $V$. The simplest idealized case of this is mapping is one-to-one, where
					each action corresponds to a single word. To test for this, we measure cooccurrence.
					The most frequently cooccurring action-word pairs $\lvert x,y\rvert$ are:</p>
				<table class="table">
					<thead>
						<th scope="col">Word lemma</th>
						<th scope="col">Action</th>
						<th scope="col">Count</th>
					</thead>
					<tbody>
						<tr><td>-pron-</td><td>19</td><td>330</td></tr>
						<tr><td>-pron-</td><td>21</td><td>189</td></tr>
						<tr><td>-pron-</td><td>20</td><td>160</td></tr>
						<tr><td>be</td><td>19</td><td>155</td></tr>
						<tr><td>the</td><td>19</td><td>100</td></tr>
						<tr><td>to</td><td>19</td><td>72</td></tr>
						<tr><td>and</td><td>19</td><td>71</td></tr>
						<tr><td>-pron-</td><td>1</td><td>70</td></tr>
						<tr><td>-pron-</td><td>23</td><td>69</td></tr>
						<tr><td>be</td><td>20</td><td>66</td></tr>
					</tbody>
				</table>
				<p>Clearly, these are biased toward frequently-occurring actions and words.
					A metric that compensates for this bias is pointwise mutual information:</p>
				<p>$$\text{pmi}(x,y)=\log\frac{p(x,y)}{p(x)p(y)}$$</p>
				<p>The highest-pmi action-word pairs are:</p>
				<table class="table">
					<thead>
						<th scope="col">Word lemma</th>
						<th scope="col">Action</th>
						<th scope="col">PMI</th>
					</thead>
					<tbody>
						<tr><td>really</td><td>5</td><td>2.107</td></tr>
						<tr><td>by</td><td>8</td><td>1.976</td></tr>
						<tr><td>what</td><td>1</td><td>1.866</td></tr>
						<tr><td>counter</td><td>6</td><td>1.766</td></tr>
						<tr><td>use</td><td>1</td><td>1.727</td></tr>
						<tr><td>first</td><td>7</td><td>1.669</td></tr>
						<tr><td>big</td><td>9</td><td>1.643</td></tr>
						<tr><td>table</td><td>8</td><td>1.634</td></tr>
						<tr><td>set</td><td>5</td><td>1.588</td></tr>
						<tr><td>bring</td><td>20</td><td>1.570</td></tr>
					</tbody>
				</table>
				<p>Unfortunately, these are mostly not verbs, and don't look like words
					that correspond intuitively to actions. Something worth noting is that
					most of these actions id's are less than 19, meaning they don't end at objects.
					Filtering to only object-endpoint actions, the highest pmi pairs are:</p>
				<table class="table">
					<thead>
						<th scope="col">Word lemma</th>
						<th scope="col">Action</th>
						<th scope="col">PMI</th>
					</thead>
					<tbody>
						<tr><td>bring</td><td>20</td><td>1.570</td></tr>
						<tr><td>over</td><td>20</td><td>1.467</td></tr>
						<tr><td>dinosaur</td><td>26</td><td>1.440</td></tr>
						<tr><td>plant</td><td>27</td><td>1.437</td></tr>
						<tr><td>roll</td><td>26</td><td>1.403</td></tr>
						<tr><td>sink</td><td>25</td><td>1.384</td></tr>
						<tr><td>next</td><td>25</td><td>1.360</td></tr>
						<tr><td>other</td><td>24</td><td>1.292</td></tr>
						<tr><td>where</td><td>25</td><td>1.230</td></tr>
						<tr><td>cut</td><td>23</td><td>1.207</td></tr>
					</tbody>
				</table>
				<p>These look a bit better, but how well do these word-action pairs actually map?
					To test these, we do a self human evaluation.</p>
			</div>
		</div>
		<div class="panel panel-default">
			<div class="panel-heading"><h2>Human Evaluation</h2></div>
			<div class="panel-body">
				<p>In this evaluation, for a given selected word-action pair, the word is presented
					with 10 randomly sampled video segments representing the corresponding action.
					Words from our target vocab are used, as well as words from max-pmi action-word pairs.</p>
				<p>For each sampled video segment, I annotate whether the word matches the video. The
					answer choices are yes, no, or unsure. Results from the target vocab are:</p>
				<div style="height: 300px">
					<canvas id="evalCanvas" width="400" height="400"></canvas>
				</div>
				<p>and from max-pmi pairs:</p>
				<div style="height: 300px">
					<canvas id="evalCanvas2" width="400" height="400"></canvas>
				</div>
				<p>Cleary this is a lot of red - this approach is not successful in finding a good
					one-to-one action-word mapping. This is with the exception of wash, which is an outlier.
					This human evaluation was done without removing actions that occur fewer than 50 times.
					Wash corresponds to action 35, which only occurs once, as a circular hand motion. "Hold" and "play" also
					have somewhat positive results, but this may be attributed to them being very
					broad terms that apply incidentally.</p>
				<p>So, what's going wrong? What is missing from the approach that is necessary to find
					a good action-word mapping? To investigate this, I'll look extensively into the
					relatively frequent and easily identifiable word-action "pick".</p>
			</div>
		</div>
		<div class="panel panel-default">
			<div class="panel-heading"><h2>Case Study &nbsp;-&nbsp; Pick</h2></div>
			<div class="panel-body">
				<p>For a deeper investigation into what's happening, I look at every
					video segment corresponding to when "pick" is said. If it's not clear that a pick action is being performed,
					I look at previous and following segments to find where the pick action is
					performed, and annotate temporal shift from when the action is said to when it's performed. For
					cases where the action spans multiple segments, I mark the segments for when the
					action starts and when it ends. The average between will be referred to as "mean shift", and the difference
					between them to "window size".</p>
				<p>In 31 of 140 instances (22.14%), the action wasn't performed. This is usually when the speaker
					is speaking abstractly, e.g. "you can use a fork to pick things up", without actually performing
					the pick action. These are excluded in the mean shift/window size charts below.</p>
				<div style="height: 250px">
					<canvas id="pickDive" width="400" height="400"></canvas>
				</div>
				<div style="height: 250px">
					<canvas id="pickDive2" width="400" height="400"></canvas>
				</div>
				<p>Qualitatively, I observed 5 clear categories. In descending order of frequency:</p>
				<ol class="list-group">
					<li class="list-group-item"><b>1. Aligned</b> They do it when they say it, often in present tense, e.g.
						"now I'm picking up X".</li>
					<li class="list-group-item"><b>2. N/A</b> Action doesn't occur.</li>
					<li class="list-group-item"><b>3. Advanced</b> They say it just before they do it, often like
						"okay now I'm going to pick up X".</li>
					<li class="list-group-item"><b>4. Delayed</b> They say it after they did it, often in sequences of actions, e.g.
						"picking up the apple and then going to put it over there".</li>
					<li class="list-group-item"><b>5. Very Delayed</b> They say it way after they did it, often in past tense, e.g.
						"earlier I picked up the apple, so now I'm going to put it over here".</li>
				</ol>
				<p>35 of 140 instances (25%) fall into the aligned category, with mean shift 0 and window size 0. These
					may be considered exemplars, where the participant is performing a pick action when they're saying pick.
					Let's look further, into the individual actions.</p>
				<h3 style="margin-top: 20px">Action Distribution</h3>
				<div style="height: 400px">
					<canvas id="pickActions" width="400" height="400"></canvas>
				</div>
				<p>The chart above compares the global action distribution, action distribution given the word
					pick, and action distribution given aligned exemplars (pick is said while being done).
					For actions 1-9, which don't end at an object, there are no exemplars.
					</p>
				<h4>Actions 1-9</h4>
				<p>There are no exemplars for actions 1-9. This is to be expected, since a pick action should end
					with the hand at an object.</p>
				<h4>Action 19</h4>
				<p>Action 19 is complicated. It has very high cooccurrence with the word pick, but less so
					for the exemplars than in general. This suggests that it cooccurs incidentally with
					pick, but may not exemplify the action. Let's verify this with examples.</p>
				<p class="text-muted" style="margin-top: 20px">Action 19, Aligned Instances:</p>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/aligned_19_7987.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_19_11839.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_19_12230.mp4" autoplay loop muted></video>
					</div>
				</div>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/aligned_19_14807.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_19_14982.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_19_15071.mp4" autoplay loop muted></video>
					</div>
				</div>
				<p class="text-muted" style="margin-top: 20px">Action 19, Random Instances:</p>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/random_19_215.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_19_4321.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_19_5328.mp4" autoplay loop muted></video>
					</div>
				</div>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/random_19_11884.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_19_12807.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_19_13292.mp4" autoplay loop muted></video>
					</div>
				</div>
				<p>We can see that action 19 is mostly just the hand hovering in place.
					Cases where it aligns with a pick action are where the pick motion is at
					the end of a period of hovering, or the motion is very slow.</p>
				<h4>Action 21</h4>
				<p>Action 21 cooccurs much more frequently with pick exemplars than otherwise. This suggests
					that it's a good example of pick. Let's look at examples of action 21.</p>
				<p class="text-muted" style="margin-top: 20px">Action 21, Aligned Instances:</p>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/aligned_21_525.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_21_1360.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_21_4966.mp4" autoplay loop muted></video>
					</div>
				</div>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/aligned_21_8056.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_21_12199.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_21_15186.mp4" autoplay loop muted></video>
					</div>
				</div>
				<p class="text-muted" style="margin-top: 20px">Action 21, Random Instances:</p>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/random_21_2503.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_21_6550.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_21_7531.mp4" autoplay loop muted></video>
					</div>
				</div>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/random_21_7533.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_21_11324.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_21_12971.mp4" autoplay loop muted></video>
					</div>
				</div>
				<p>Action 21 seems to be a motion where the hand is moving in the direction
					of the thumb. It seems to be the best example of a pick motion. However,
					looking at the random instances of action 21, these are typically extremely
					short, often only a single frame at the 10Hz BP-AR-HMM input rate.</p>
				<h4>Action 23</h4>
				<p>Action 23 is distributed similarly to Action 21, just being less frequent
					in all cases. Let's see what it looks like.</p>
				<p class="text-muted" style="margin-top: 20px">Action 23, Aligned Instances:</p>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/aligned_23_569.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_23_4645.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/aligned_23_7050.mp4" autoplay loop muted></video>
					</div>
				</div>
				<p class="text-muted" style="margin-top: 20px">Action 23, Random Instances:</p>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/random_23_168.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_23_809.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_23_3159.mp4" autoplay loop muted></video>
					</div>
				</div>
				<div class="row">
					<div class="col-sm-4">
						<video src="src/random_23_7106.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_23_10100.mp4" autoplay loop muted></video>
					</div>
					<div class="col-sm-4">
						<video src="src/random_23_10551.mp4" autoplay loop muted></video>
					</div>
				</div>
				<p>In clips of Action 23, it looks like the hand is lifting while rotating
					outward toward the thumb. Again, in the random examples, most samples
					are very short. Something also noteworthy is that in the bottom right
					example, it's a left-handed motion. Hoever, the left hand motion isn't
					mirrored. This is a problem - left-handed motions should be mirrored,
					so our data needs to be re-preprocessed accordingly.</p>
				<h4>Actions Summary</h4>
				<p>Actions appear to be internally consistent, and represent meaningful
					autoregressive models. However, many instances are extremely short.
					Here are ideas on how pick can be captured successfully with the
					current actions:<p>
				<ol class="list-group">
					<li class="list-group-item"><b>1. Action 21/23</b> The pick motion
						seems to be best represented by Action 21, and maybe Action
						23. The duration of these actions are important. A long
						Action 21 should be more confidentally identifiable as pick. Other
						actions, such as 1-9 or 19, should never be considered pick.</li>
					<li class="list-group-item"><b>2. Temporal Window</b> A crucial element
						is the fact that the pick motion should be preceded by a
						non-object-ending action (1-9). That is, the hand isn't on an
						object before a pick action. However, between the preceding 1-9
						and the pick motion (21), there will likely be a sizeable Action 19,
						as the hand is hovering over the object. So, we will likely need a temporal
						window that looks back at least 2 actions to consistently identify "pick".</li>
					<li class="list-group-item"><b>3. Object Motion</b> Related to the previous point
						is object motion. Though the presence of object motion is somewhat
						inferrable through 1-9 preceding 19+, it's not always the case. For example,
						in the lower center example of Random Action 23, the hand is moving over the doll,
						but isn't actually holding the doll. A solution to this, which may be crucial
						to other verbs like roll, is to also represent object-centric actions. The temporal
						window may also be simplified in this case, since the hand hovering over an object
						wouldn't change the object state. The object state wouldn't change until it starts
						moving.</li>
				</ol>
				<p>In other words, a pick action is often expressed by the following action sequence:</p>
				<p>$$\textit{pick} = A_{1-9}\rightarrow A_{19}^\ast\rightarrow A_{21}$$</p>
				<p>An ideal model should weight confidence according to length and identity
					of each action, and down the road, take into account object-centric actions.</p>
				<h4>Future Work</h4>
				<p><b>Preprocessing.</b> Mirror left-handed data; adjust BP-AR-HMM parameters?</p>
				<p><b>Temporality.</b> Somehow, we need to detect sequences. 25% of pick utterances
					are aligned with a pick action. However, these are crucially preceded by
					certain other actions, variable in how far back-shifted these other actions are.
					How do we attend to these sequence patterns, alongside 75% of utterances being not
					aligned?</p>
				<p><b>Object-centricity.</b> Parallel object-centric actions will be important
					in detecting actions, and might be able to make temporality easier.</p>
			</div>
		</div>
	</div>
	<footer class="page-footer">
		<div class="footer-copyright text-center py-3"><a href="http://dylanebert.com">Dylan Ebert</a></div>
	</footer>
</div>
</body>
<script>
var ctx = $('#actionDistributionCanvas')
var actionDistribution = new Chart(ctx, {
	type: 'bar',
	data: {
		labels: ['1', '2', '3', '4', '5', '6', '7', '8', '9'],
		datasets: [{
			label: '# of Occurrences',
		    data: [3423, 2134, 4484, 904, 1707, 886, 1084, 1237, 829],
			backgroundColor: '#e74c3c'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'action id'
				}
			}]
		}
	}
})

var ctx = $('#endDistributionCanvas')
var endDistribution = new Chart(ctx, {
	type: 'bar',
	data: {
		labels: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '19', '20', '21', '22', '23', '24', '25', '26', '27'],
		datasets: [{
			label: '# of Occurrences',
		    data: [683, 692, 835, 252, 249, 365, 371, 338, 300, 2740, 1442, 3649, 652, 1458, 521, 713, 899, 529],
			backgroundColor: '#3498db'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'action id (endpoint-clustered)'
				}
			}]
		}
	}
})

var ctx = $('#wordsCanvas')
var endDistribution = new Chart(ctx, {
	type: 'bar',
	data: {
		labels: ['-PRON-', 'be', 'the', 'and', 'to', 'go', 'a', 'up', 'put', 'so', 'that', 'have', 'on', 'pick', 'in'],
		datasets: [{
			label: '# of Occurrences',
		    data: [1283, 604, 316, 316, 301, 228, 196, 182, 177, 174, 150, 147, 142, 140, 139],
			backgroundColor: '#9b59b6'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'most frequent words'
				}
			}]
		}
	}
})

var ctx = $('#ourWordsCanvas')
var endDistribution = new Chart(ctx, {
	type: 'bar',
	data: {
		labels: ['pick', 'put', 'get', 'drop', 'throw', 'hold', 'go', 'take', 'eat', 'drink', 'wash', 'play', 'cut', 'peel', 'roll'],
		datasets: [{
			label: '# of Occurrences',
		    data: [140, 177, 81, 18, 24, 22, 228, 60, 76, 22, 30, 41, 7, 21, 13],
			backgroundColor: '#34495e'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'target words'
				}
			}]
		}
	}
})

var ctx = $('#evalCanvas')
var endDistribution = new Chart(ctx, {
	type: 'bar',
	data: {
		labels: ['put', 'get', 'drop', 'throw', 'hold', 'go', 'take', 'eat', 'drink', 'wash', 'play', 'cut', 'peel', 'roll'],
		datasets: [{
			label: 'Proportion Yes',
		    data: [0.0, 0.111, 0.0, 0.0, 0.615, 0.0, 0.0, 0.0, 0.0, 0.888, 0.2, 0.0, 0.0, 0.0],
			backgroundColor: '#2ecc71'
		}, {
			label: 'Proportion Unsure',
			data: [0.1, 0.444, 0.1, 0.0, 0.0769, 0.5, 0.3, 0.1, 0.0, 0.111, 0.2, 0.0, 0.0, 0.1],
			backgroundColor: '#f1c40f'
		}, {
			label: 'Proportion No',
			data: [0.9, 0.444, 0.9, 1.0, 0.307, 0.5, 0.7, 0.9, 1.0, 0.0, 0.6, 1.0, 1.0, 0.9],
			backgroundColor: '#e74c3c'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'target words'
				}
			}]
		}
	}
})

var ctx = $('#evalCanvas2')
var endDistribution = new Chart(ctx, {
	type: 'bar',
	data: {
		labels: ['away', 'because', 'big', 'bring', 'by', 'counter', 'cut', 'drink', 'dry', 'first', 'like', 'other', 'plant', 'really', 'roll', 'sink', 'truck', 'wash'],
		datasets: [{
			label: 'Proportion Yes',
		    data: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.777],
			backgroundColor: '#2ecc71'
		}, {
			label: 'Proportion Unsure',
			data: [0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.05, 0.0, 0.3, 0.1, 0.0, 0.1, 0.0, 0.0, 0.3, 0.2, 0.0, 0.222],
			backgroundColor: '#f1c40f'
		}, {
			label: 'Proportion No',
			data: [1.0, 1.0, 1.0, 0.7, 1.0, 1.0, 0.95, 1.0, 0.7, 0.9, 1.0, 0.9, 1.0, 1.0, 0.7, 0.8, 1.0, 0.0],
			backgroundColor: '#e74c3c'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'max-pmi words'
				}
			}]
		}
	}
})

var ctx = $('#pickDive')
var endDistribution = new Chart(ctx, {
	type: 'line',
	data: {
		labels: [-20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],
		datasets: [{
			label: '# of Instances',
		    data: [1, 2, 0, 0, 0, 1, 1, 0, 3, 1, 3, 0, 2, 2, 2, 3, 5, 3, 2, 8, 52, 7, 5, 2, 3, 1],
			backgroundColor: '#34495e'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'mean shift (# of segments)'
				}
			}]
		}
	}
})

var ctx = $('#pickDive2')
var endDistribution = new Chart(ctx, {
	type: 'line',
	data: {
		labels: [0, 1, 2, 3, 4, 5],
		datasets: [{
			label: '# of Instances',
		    data: [39, 33, 23, 7, 5, 2],
			backgroundColor: '#34495e'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'window size (# of segments)'
				}
			}]
		}
	}
})

var ctx = $('#pickActions')
var actionDistribution = new Chart(ctx, {
	type: 'line',
	data: {
		labels: [1, 2, 3, 6, 7, 8, 9, 19, 20, 21, 22, 23, 24, 25, 26, 27],
		datasets: [{
			label: 'Proportion action (global)',
			data: [0.042, 0.043, 0.052, 0.023, 0.023, 0.021, 0.019, 0.169, 0.089, 0.225, 0.040, 0.090, 0.032, 0.044, 0.056, 0.033],
			fill: false,
			borderColor: '#9b59b6'
		}, {
			label: 'Proportion action given word pick (total)',
		    data: [0.029, 0.029, 0.021, 0.021, 0.007, 0.036, 0.007, 0.407, 0.064, 0.179, 0.021, 0.071, 0.029, 0.036, 0.029, 0.014],
			fill: false,
			borderColor: '#3498db'
		}, {
			label: 'Proportion action given pick exemplars (aligned)',
			data: [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.314, 0.057, 0.371, 0.029, 0.114, 0.000, 0.057, 0.029, 0.029],
			fill: false,
			borderColor: '#2ecc71'
		}]
	},
	options: {
		reponsive: true,
		maintainAspectRatio: false,
		scales: {
			yAxes: [{
				ticks: {
					beginAtZero: true
				}
			}],
			xAxes: [{
				scaleLabel: {
					display: true,
					labelString: 'action id'
				}
			}]
		}
	}
})
</script>
</html>
