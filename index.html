<html lang="en">
<head>
	<meta charset="utf-8">
	<title>ReAI Bayesian</title>
	<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-colorschemes"></script>
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    MathJax = {
    	tex: {
        	inlineMath: [['$', '$'], ['\\(', '\\)']]
    	},
		svg: {
			fontCache: 'global'
		}
    };
    </script>
	<script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
	<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
	<link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.21/css/jquery.dataTables.css">
</head>
<style>
html, body {
	font-family: 'Open Sans', sans-serif;
	width: 100%;
	height: 100%;
}
p {
	font-size: 1.3em;
}
.head {
	padding-top: 50px;
	padding-bottom: 10px;
	padding-left: 30px;
	position: relative;
	width: 100%;
}
h1 {
	letter-spacing: .5px;
	font-size: 3.5em;
}
h2 {
	font-weight: bold;
	font-size: 1.8em;
}
h3 {
	font-weight: bold;
	font-size: 1.5em;
}
h4 {
	font-weight: bold;
	margin-top: 20px;
}
.page-footer {
	padding: 50px;
	background-color: #eeeeee;
}
video {
	max-width: 100%;
	height: auto;
}
</style>
<body>
<div class="container">
	<div class="head">
		<h1>ReAI</h1>
	</div>
	<div class="panel-group">
		<p>Reintegrating nonparametric bayesian models for grounded language acquisition.</p>
		<div class="panel panel-default">
			<h2>Introduction</h2>
			<p>This write-up is a first-hand account of my dive into the world of nonparametric bayesian models for the eventual goal of
				obtaining grounded representations of actions (verbs), so robots can better understand what verbs <i>really</i> mean, in terms of our physical
				reality.</p>
			<p>Initially, this was meant to be a well-wrapped box of research findings - a novel approach to grounding actions, and results. However,
				what was meant to be a simple statistically-driven abstraction of spatiotemporal signals ended up being a semester-long seminar on
				nonparametric bayesian world.</p>
			<p>In this write-up, I'll log my experience working on this project, notably from the perspective of an NLP researcher born academically post-neural-networks.
				It may help another researcher who's looking into this bayesian stuff, or at least serve as my own project documentation.</p>
		</div>
		<div class="panel panel-default">
			<h2>Project Goals</h2>
			<div class="row">
				<div class="col-sm-8">
					<p>The top-level goal of this project is to make robots understand actions the way people do.</p>
					<p>We don't know exactly how people understand things, but our best guess is that there are many layers of abstraction between sensory data (seeing, hearing, etc.)
						and thought. Thought is communicated with words, some of which are verbs. In today's state-of-the-art NLP models, verbs aren't represented as a series of abstractions
						from sensor data. Rather, they are represented in terms of other words. This isn't sufficient for a robot to link up language with the world. So, this research
						is interested in building these abstractions, from sensing to verbs.</p>
					<p>Perhaps the most difficult element in this series of abstractions is the first one, from raw sensory data (e.g. pixels) to something a bit simpler, such as objects
						with properties. This problem encompasses, in fact, a majority of computer vision research. Since I'm more interested in the less-researched, higher layers of abstraction,
						I decide to cheat, and start with data already comprised of objects with properties. This abstraction somewhat trivializes noun learning, but leaves a lot of
						unexplored territory in verb learning.</p>
					<p>Verbs happen over time, and depend on dynamic object properties. Think about the difference between throwing and dropping a ball.
						How can this be explained in terms of how the ball's (and agent's) properties change over time?</p>
					<p>The low-level goal of this project is to build a second layer of abstraction,
						from objects and their properties, to a higher-level representation. These higher-level representations would ideally resemble low-level verbs, or be able to
						be combined in a way that resemble low-level verbs.</p>
				</div>
				<div class="col-sm-4">
					<img src="figure1.png" style="max-width: 85%"></img>
				</div>
			</div>
		</div>
		<div class="panel panel-default">
			<h2>The Data</h2>
			<p>Since most existing grounded language datasets use images/videos, and I want to start with a dataset of objects and their properties, I use data we've collected of people
				narrating as they do everyday tasks in VR. More examples from this dataset can be explored at <a href="http://newbrowncorpus.appspot.com">
				http://newbrowncorpus.appspot.com</a>.</p>
			<div style="text-align: center; margin-bottom: 20px;">
				<video src="examplevid.mp4" controls>
			</div>
			<p>In this case, "objects and their properties" consists of position, rotation, and bounding boxes of all objects in the scene, recorded at approximately 90Hz.
				The head and hands are also recorded as objects.</p>
			<p>So, how do I convert position and rotation of objects over time into something closer to words like "throw" and "pick up"? The first step in this is, of course, to
				see what other people have recently done. It turns out, not much. At least, not much given some challenging constraints: <b>unsupervised</b>, <b>limited data</b>,
				and <b>being fine-grained 3d spatiotemporal signals</b>. Common time-series problems like stock prediction, GPS-based activity recognition, video classification, etc. don't
				transfer well to this problem as a result.</p>
			<p>Prior to looking into the nonparametric bayesian approaches I focus on here, I spent a few months trying popular neural network approaches - RNNs, CNNs, and transformers. I won't go into detail
				on everything I tried with these, but after months of rigorously testing variations of these models, nothing was working. One of the most glaring reasons why this may be is that our
				dataset is very small - many orders of magnitude smaller than comparable language and vision models. I wasn't willing to accept that the dataset is too small and move on, though,
				since children are shown to be able to learn from few examples. So, I asked around, and was pointed toward nonparametric bayesian models.</p>
		</div>
		<div class="panel panel-default">
			<h2>Nonparametric Bayesian Models</h2>
			<p>Some years ago, prior to the popularization of neural networks, Bayesian models were a much more popular way to approach this kind of stuff.
				The work I found to be by far the most relevant to this project is <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386006&casa_token=OzGWIvYc6vwAAAAA:LwezTLnWvieXcjRDYJ7gI246y9z8HFG8hc5ihrafapFqwct6MsVD-RR8kITcdwqVMjujwtGueUk&tag=1">
				Learning and Generalization of Complex Tasks from Unstructured Demonstrations</a>, which uses the nonparametric Bayesian model BP-AR-HMM to segment and
				label trajectories. This is the Beta Process Autoregressive Hidden Markov Model. It seemed to suit my problem perfectly, so I decided to try it.</p>
			<p>I started by downloading the original <a href="http://michaelchughes.github.io/NPBayesHMM/">matlab implementation</a> of BPARHMM and running it on my data. I didn't understand what
				anything meant, and kept all hyperparameters as-is. This turned out to be a mistake. With neural networks, it's fathomable to get some degree of success without actually
				understanding what the network is doing, or what the hyperparameters do. However, with bayesian models, it's important to understand what these parameters mean. In my case,
				learning matlab, bayesian concepts, and this particular model at the same time, was not feasible.</p>
			<p>So, my next step was to switch to the <a href="https://github.com/bnpy/bnpy/">bnpy toolbox</a>, a python toolbox for bayesian nonparametric models. From this, I was able
				to gain a much better understanding of how to use these models. The bnpy toolbox is structured in terms of three modules: an allocation model, an observation model, and a learning
				algorithm.</p>
			<p>The allocation model is what papers spend all their time talking about, and include things like Hidden Markov Models and Mixture Models. BP-AR-HMM would be considered
				an allocation model. One concern I had about using this toolbox was that it doesn't include BP-AR-HMM; the most similar is the HDPHMM, or Hierchical Dirichlet Process
				Hidden Markov Model, which in fact doesn't have autoregressive in the name. I want an autoregressive model, since position should depend upon previous position. However, this
				turned out not to be an issue, since autoregression isn't actually the job of the allocation model. This was difficult to conceptualize from my perspective, since I didn't
				know there was more to the story than the allocation model. There is, though: the observation model.</p>
			<p>The observation model is responsible for how the allocation model actually outputs stuff. This seems pretty obvious when described
				in terms of a simple hidden markov model: a hidden markov model consists of a set of hidden states with initial and transition probabilities - these are the allocation model. Each of these states
				emits some values, such as samples from a gaussian. This is the observation model. Specifically, in this example, it's a gaussian observation model.
				The observation model isn't talked about much, in educational resources nor research papers, but isn't trivial in practice. The bnpy toolbox made this much easier to understand with its dichotomy,
				with various choices for both allocation models and observation models. In my case, the right choice was an <i>autoregressive</i> gaussian observation model, which emits
				values based previous values and the hidden state.</p>
			<p>There are a lot of hyperparameters for both the allocation model and the observation model, and they matter. The allocation model hyperparameters were easier to figure out, since
				research and documentation emphasize them. For example, <i>kappa</i> in the HDPHMM allocation model is the self-stickyness hyperparameter, or the propensity for states to stay in the same
				state, resulting in longer sequences. This ended up being the hyperparameter I search across in experiments.</p>
			<p>Observation model hyperparameters, on the other hand, are much more obscure. They are generally described as parameters of the "inverse wishart prior" which, for me, is unfortunately not too
				meaningful. My understanding is that, because you can't "just backpropogate" emission parameters, you need to do something else to figure out what they should be. The preferred answer to that
				in this case is the inverse wishart prior. In my case, figuring out exactly what the inverse wishart prior is doing quickly led to a rabbit hole of math that seemed to go far beyond
				what's preferable for just running these nonparametric bayesian methods on my data.</p>
			<p>As far as I've found, there isn't really an easy solution to this, except to figure out what you can from reading, and then figure out the rest by turning knobs and seeing what happens.
				I ended up scaling my data to zero-mean unit-variance, and using similar parameters to another autoregressive gaussian example, plus some small-scale hyperparameter searching.</p>
		</div>
		<div class="panel panel-default">
			<h2>Neural Approach</h2>
			<div class="row">
				<div class="col-sm-8">
					<p>In parallel to the bayesian approach, I also go through all experiments with a simple neural network approach: an autoencoder, followed by K-means clustering.</p>
					<p>More specifically, I trained an autoencoder on chunks of right-hand position data. Since this approach requires fixed-length inputs, I split the data evenly into
						non-overlapping half-second chunks. I then train a convolutional autoencoder, consisting of an encoder, which encodes each half-second chunk into a
						low-dimensional vector, and a decoder, which can decode this vector
						back to the original. I try this with various <i>latent sizes</i>, including 2, 3, and 4 dimensions. Higher latent sizes were tried, but led to
						curse of dimensionality issues in the following clustering step.</p>
					<p>I run these half-second chunks through the encoder to obtain vectors. I use K-Means clustering to cluster these vectors: a simple unsupervised
						clustering method that requires a predefined <i>K</i> number of clusters. I try this for various number of clusters <i>K</i>, including 20, 50, 100, 200, and 500.
						So, a two-dimensional hyperparameter search is applied for variations on <i>latent size</i> and <i>K</i>.</p>
					<p>This approach is limited in that, unlike the nonparametric bayesian approach, it requires fixed-length inputs, and a predefined number of clusters. So, this approach
						is treated as a baseline for comparison, rather than as a competing model.</p>
				</div>
				<div class="col-sm-4">
					<img src="figure2.png" style="max-width: 70%; margin-bottom: 20px;"></img>
				</div>
			</div>

		</div>
		<div class="panel panel-default">
			<h2>Training</h2>
			<!--add tuning data video-->
			<p>To help me debug and tune hyperparameters, I created a small supervised dataset that I'll refer to as the <i>tuning</i> dataset. In this, I record three sessions of
				myself reaching for objects, then dropping them. I annotate these with three action states: idle, reach and lift. This makes it easy to approximate which hyperparameters
				to tune on the full dataset, and what range of values is appropriate for these hyperparameters.</p>
			<p>I mentioned earlier that our dataset is very small. This is relative to modern NLP models. However, compared to previous works' experiments in bayesian world, it's very large.
				Training one hyperparameter configuration with the bayesian approach on the full dataset took about 24 hours, and produced unusual results, such as having over 200 unique states.</p>
			<p>To address this problem, I used a subset of the full dataset, taking only 6 out of 18 participants, and 3 out of 6 tasks. Across various hyperparameter settings,
				training on this subset converged to a much smaller number of hidden states (about 30). I'm still unsure why there's such a large variation with respect to dataset size.</p>
			<p>For consistency, I train the neural network approach with the same partial dataset, though it only takes a few minutes to train in either case.</p>
		</div>
		<div class="panel panel-default">
			<h2>Results</h2>
			<p>One issue in designing experiments for this project is that there isn't a known right answer for what the abstraction from
				objects to actions is supposed to look like. A good result could be states that align well to low-level verbs, or short trajectories that can be combined to represent verbs.</p>
			<p>So, I use a quantitative evaluation that doesn't strictly dictate performance, but may indicate whether these action representations correlate with verb semantics. That evaluation is:</p>
			<div class="row">
				<div class="col-sm-6">
					<ul class="list-group" style="margin-bottom: 20px">
						<li class="list-group-item"><b>1. </b>Given the list of words and when the a participant says them, filter to a list of target verbs.</li>
						<li class="list-group-item"><b>2. </b>For each of these verbs, take the current learned action representation. For example, the current action may be
							<b>3</b> when the word "put" is said, corresponding to state or cluster with arbitrary id <b>3</b>.</li>
						<li class="list-group-item"><b>3. </b>Construct an ngram feature vector from the current action and surrounding actions. So if the surrounding are actions 21342, then a window=1 ngram feature vector would contain
							1, 3, 4, 13, 34, and 134. Each of these can be treated as a term in a term frequency vector.</li>
						<li class="list-group-item"><b>4. </b>Given each ngram vector corresponding to target verbs spoken in train, find the most similar dev-dataset ngram. Compare the word being spoken in this vector to the word being
							spoken in the train example. If it's the same, consider it a positive result. If it's not, consider it a negative result. In other words, do similar action sequences correspond
							to similar spoken verbs?</li>
						<li class="list-group-item"><b>5. </b>In cases where there are multiple dev ngrams that are equally similar to the train ngram, take the proportion equivalent verbs as an accuracy entry.</li>
					</ul>
				</div>
				<div class="col-sm-6">
					<img src="figure3.png" style="max-width: 100%"></img>
				</div>
			</div>
			<p>This evaluation doesn't strictly dictate performance because, people don't always say exactly what they're doing. They may use a word temporally removed from their actions, e.g.
				"I picked up an apple earlier". Or, they may perform an action without actually saying the verb. However, the hope is that this metric should trend higher the better the model's
				action representations align to spoken verbs.</p>
			<table id="resultsTable" style="margin-bottom: 20px">
				<thead>
					<tr>
						<th>Method</th>
						<th>Accuracy (Macro)</th>
						<th>Accuracy (Micro, excl. "go")</th>
					</tr>
				</thead>
				<tr>
					<td><b>random baseline</b></td>
					<td><b>.011</b></td>
					<td><b>.005</b></td>
				</tr>
				<tr>
					<td>bnpy kappa=10, window=1</td>
					<td>.017</td>
					<td>.002</td>
				</tr>
				<tr>
					<td>bnpy kappa=10, window=2</td>
					<td>.014</td>
					<td>0</td>
				</tr>
				<tr>
					<td>bnpy kappa=50, window=1</td>
					<td>.023</td>
					<td>.002</td>
				</tr>
				<tr>
					<td>bnpy kappa=50, window=2</td>
					<td>.036</td>
					<td>.003</td>
				</tr>
				<tr>
					<td>bnpy kappa=50, window=3</td>
					<td>.017</td>
					<td>.003</td>
				</tr>
				<tr>
					<td>bnpy kappa=100, window=1</td>
					<td>.014</td>
					<td>.004</td>
				</tr>
				<tr>
					<td>bnpy kappa=100, window=2</td>
					<td>.012</td>
					<td>.006</td>
				</tr>
				<tr>
					<td>nn latent=2, kmeans=100</td>
					<td>.015</td>
					<td>.020</td>
				</tr>
				<tr>
					<td>nn latent=3, kmeans=200</td>
					<td>.014</td>
					<td>.018</td>
				</tr>
				<tr>
					<td>nn latent=4, kmeans=100</td>
					<td>.019</td>
					<td>.005</td>
				</tr>
				<tr>
					<td>nn latent=4, kmeans=200</td>
					<td>.016</td>
					<td>.016</td>
				</tr>
			</table>
			<p>The "random baseline" in this case is just the probability of randomly choosing the same verb in the dev set. <b>Macro</b> is the average accuracy across all instances.
				<b>Micro</b> is the average-of-averages for each word. I exclude the word "go" in this, since "go" is very frequently used for future tense (i.e. going to drink), resulting
				in a high chance of spurious positive results.</p>
			<p>Macro accuracy across all examples are higher than the random baseline, which is promising. However, not by a large margin. To look into cases where it's succeeding and
				failing, I looked at accuracy on a per-word basis, and found "go" was consistently skewing macro accuracy upward. Micro accuracy, excluding "go", was only above the random
				baseline in all but one neural network case, and only one bayesian case. On per-word investigation of the neural cases, "take" is consistently pulling the accuracy above
				random, while it's 0 for other words.</p>
			<p>I want to know what these actions look like, so I look at examples of actions from the top-performing neural and bayesian models, using the interface
				<a href="http://newbrowncorpus.appspot.com/actions">here</a>. In general, the bayesian actions seem to have the hand following similar trajectories. However,
				I wouldn't be able to look at a sample and say what action it is. Some actions look similar, and there are many that have motion I wouldn't consider meaningful.</p>
			<p>With the neural approach, the trajectories generally don't look similar. However, the hand tends to hover in a similar relative position. Though the convolutional
				autoencoder should be capable of encoding change over time, it's likely prioritizing the area the hand is in. This indicates that this approach clearly isn't
				encoding actions well, and above-random accuracy in this case isn't too meaningful.</p>
		</div>
		<div class="panel panel-default">
			<h2>Conclusion</h2>
			<p>Neither approach succeeded in capturing actions that map straightforwardly to verb semantics, as far as this evaluation can suggest. This isn't to say that
				the target representations are definitely bad. It may be that more abstraction is needed to reach verb semantics.</p>
			<p>It's my intuition that hand position alone isn't enough - that object positions need to be taken into account to get good representations. However, there isn't
				an obvious way how to do this, since the salient object(s) vary over time, and decisions would have to be made about which objects matter and when. It's also not
				to say that this intuition is necessarily right - there may just a missing piece between the current bayesian states and verbs.</p>
			<p>In any case, through this project, I was introduced to bayesian methods I wouldn't otherwise have been exposed to, which may come in handy down the road, especially
				with what appears to be a recent reemergence of bayesian approaches within neural networks.</p>
		</div>
	</div>
	<footer class="page-footer">
		<div class="footer-copyright text-center py-3"><a href="http://dylanebert.com">Dylan Ebert</a></div>
	</footer>
</div>
</body>
<script>
$(document).ready(function() {
	$('#resultsTable').DataTable({
		'searching': false,
		'paging': false,
		'order': [[2, 'desc']],
		'bInfo': false
	})
})
</script>
</html>
